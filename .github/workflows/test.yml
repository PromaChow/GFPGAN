name: GPU CI

on:
  push:
    branches: [main, master]
  pull_request:
  workflow_dispatch:

jobs:
  gpu-tests:
    # Use your self-hosted GPU runner
    runs-on: [self-hosted, gpu, linux]

    env:
      TORCH_CUDA_CHANNEL: "https://download.pytorch.org/whl/cu121"

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Free up disk space (just in case)
        run: |
          echo "Before cleanup:"
          df -h
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/lib/android /opt/hostedtoolcache/CodeQL || true
          sudo apt-get clean
          echo "After cleanup:"
          df -h

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install CUDA-enabled PyTorch
        run: |
          pip install --upgrade pip
          pip install --extra-index-url "$TORCH_CUDA_CHANNEL" torch torchvision torchaudio

      - name: Install project dependencies
        run: |
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          fi
          pip install pytest gfpgan

      - name: Verify CUDA availability
        run: |
          python - <<'PY'
          import torch
          assert torch.cuda.is_available(), "CUDA not available!"
          print("âœ… CUDA available with", torch.cuda.device_count(), "device(s)")
          print("Device:", torch.cuda.get_device_name(0))
          PY

      - name: Run GPU tests
        run: pytest tests -v --maxfail=1 --durations=0
